
<!DOCTYPE html>
<!-- saved from url=(0033)http://web.cs.ucla.edu/~ahmedk/ -->
<html lang="en"><head><script src="//archive.org/includes/athena.js" type="text/javascript"></script>
<script type="text/javascript">window.addEventListener('DOMContentLoaded',function(){var v=archive_analytics.values;v.service='wb';v.server_name='wwwb-app212.us.archive.org';v.server_ms=414;archive_analytics.send_pageview({});});</script>
<script type="text/javascript" src="https://web-static.archive.org/_static/js/bundle-playback.js?v=HxkREWBo" charset="utf-8"></script>
<script type="text/javascript" src="https://web-static.archive.org/_static/js/wombat.js?v=txqj7nKC" charset="utf-8"></script>
<script>window.RufflePlayer=window.RufflePlayer||{};window.RufflePlayer.config={"autoplay":"on","unmuteOverlay":"hidden"};</script>
<script type="text/javascript" src="https://web-static.archive.org/_static/js/ruffle/ruffle.js"></script>
<script type="text/javascript">
    __wm.init("https://web.archive.org/web");
  __wm.wombat("https://web.cs.ucla.edu/~ahmedk/","20240430001039","https://web.archive.org/","web","https://web-static.archive.org/_static/",
	      "1714435839");
</script>
<link rel="stylesheet" type="text/css" href="https://web-static.archive.org/_static/css/banner-styles.css?v=S1zqJCYt" />
<link rel="stylesheet" type="text/css" href="https://web-static.archive.org/_static/css/iconochive.css?v=qtvMKcIJ" />
<!-- End Wayback Rewrite JS Include -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Kareem Ahmed</title>

  <!-- Bootstrap core CSS -->
  <link href="files/bootstrap.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="files/small-business.css" rel="stylesheet">
  <link rel="stylesheet" href="files/all.css">
  <link rel="stylesheet" href="files/academicons.css">
  <style type="text/css">
    .content {
      margin-top: 0;
      margin-bottom: 0
    }
  </style>
  
  <meta name="google-site-verification" content="qEih9m0y-6X0QuisQYfHSxOvkW-o5Q3dfxuQ5Z4JtGA">

</head>
  <!-- Page Content -->
  <div class="container">
    <div class="row mb-3">
      <div class="col">
          <h1>Kareem Ahmed</h1>
      </div>
    </div>
    <div class="row my-4">
      <!-- /.col-lg-8 -->
      <div class="col-lg-8">
        <p class="m-0">
        I am a Ph.D. candidate in the Computer Science department at the
        University of California, Los Angeles (UCLA), where i'm co-advised by
        <span style="color:#4169E1"><a href="https://web.archive.org/web/20240430001039/http://web.cs.ucla.edu/~guyvdb/">
        Guy Van den Broeck</a></span> and <span style="color:#4169E1"><a href="https://web.archive.org/web/20240430001039/http://web.cs.ucla.edu/~kwchang/"> Kai-Wei Chang</a></span>.
        </p><p></p>
        <!--p>
        I'm broadly interested in Neuro-Symbolic AI. In particular, i'm interested
        in tractable probabilistic learning and reasoning in output spaces whose
        structure can be characterized by logical constraints, and where the outputs
        of a neural network induce a probability distribution over the output space.
        </p-->
        <p>
        My research interests lie broadly in artificial intelligence and machine learning.
        My aim is to exploit the unprecedented capabilities of current architectures,
        such as <strong>transformers and large language models</strong>, to learn from data while endowing
        them with <strong>logical reasoning capabilities</strong> to yield <strong>trustworthy</strong> and <strong>explainable</strong> systems.
        I thus view machine learning systems as inducing probability distributions over output
        spaces and develop sound and tractable algorithms to reason about them.
        </p>
        <p>Email: ahmedk [at] cs [dot] ucla [dot] edu </p>
        <p>Research Interests:<ul><li>Neuro-Symbolic AI</li> <li>Trustworthy AI</li><li>Tractable Probabilistic Modeling</li><li>Knowledge Representation and Reasoning</li></ul>
        <p>
        Links:
        [<a href="https://web.archive.org/web/20240430001039/https://scholar.google.com/citations?user=hkM0hbIAAAAJ">Google Scholar</a>]
        [<a href="https://web.archive.org/web/20240430001039/https://github.com/KareemYousrii/">Github</a>]
        [<a href="https://web.archive.org/web/20240430001039/https://twitter.com/KareemYousrii">Twitter</a>]
        </p>
        <strong><span style="color:red">
          I am on the 2023-2024 academic job market! Please reach out if you have any openings!
        </span></strong>
      </div>
      <div class="col-lg-4">
        <img class="img-fluid rounded img-responsive" style="width:60%" src="files/profile-pic.jpg" alt="">
      </div>
      <!-- /.col-md-4 -->
    </div>
    <!-- /.row -->

    <div class="my-4 text-left">
      <h2>Publications</h2>
      <ul>
          <li><b><a href="https://web.archive.org/web/20240430001039/https://openreview.net/forum?id=Tj6Wcx7gVk">Probabilistically Rewired Message-Passing Neural Networks</a></b>
          <p class="content"> Chendi Qian*, Andrei Manolache*, <strong>Kareem Ahmed</strong>, Zhe Zeng, Guy Van den Broeck, Mathias Niepert*, Christopher Morris*</p>
          <!-- <br> -->
            <em>ICLR 2024</em>.
          <br>
          [<a href="https://web.archive.org/web/20240430001039/http://web.cs.ucla.edu/~ahmedk/" onclick="$('#pr-mpnns-abstract').toggle();return false;">abstract</a>]
          <div id="pr-mpnns-abstract" class="abstract" style="display:none;">
              <p>
                Message-passing graph neural networks (MPNNs) emerged as powerful tools for
                processing graph-structured input. However, they operate on a fixed input graph
                structure, ignoring potential noise and missing information. Furthermore, their
                local aggregation mechanism can lead to problems such as over-squashing and
                limited expressive power in capturing relevant graph structures. Existing solutions
                to these challenges have primarily relied on heuristic methods, often disregarding
                the underlying data distribution. Hence, devising principled approaches for learning
                to infer graph structures relevant to the given prediction task remains an open
                challenge. In this work, leveraging recent progress in exact and differentiable ksubset sampling, we devise probabilistically rewired MPNNs (PR-MPNNs), which
                learn to add relevant edges while omitting less beneficial ones. For the first time,
                our theoretical analysis explores how PR-MPNNs enhance expressive power, and
                we identify precise conditions under which they outperform purely randomized
                approaches. Empirically, we demonstrate that our approach effectively mitigates
                issues like over-squashing and under-reaching. In addition, on established realworld datasets, our method exhibits competitive or superior predictive performance
                compared to traditional MPNN models and recent graph transformer architectures.
              </p>
          </div>
          <p></p>
        </li>

          <li><b><a href="https://web.archive.org/web/20240430001039/https://openreview.net/pdf?id=hVAla2O73O">A Pseudo-Semantic Loss for Deep Generative Models with Logical Constraints</a></b>
          <p class="content"> <strong>Kareem Ahmed</strong>, Kai-Wei Chang, Guy Van den Broeck </p>
          <!-- <br> -->
            <em>NeurIPS 2023</em>.
          <br>
          [<a href="https://web.archive.org/web/20240430001039/http://web.cs.ucla.edu/~ahmedk/" onclick="$('#pseudo-sl-abstract').toggle();return false;">abstract</a>]
          <div id="pseudo-sl-abstract" class="abstract" style="display:none;">
              <p>
                Neuro-symbolic approaches bridge the gap between purely symbolic and neural approaches to learning.
                <!-- -->
                This often requires maximizing the probability of a symbolic constraint in the neural network's output. However, output distributions are typically assumed to be fully-factorized, which prohibits the application of neurosymbolic learning to more expressive output 
                distributions, such as autoregressive deep generative models. There, such probability computation is #P-hard, 
                even for simple constraints.
                <!-- -->
                Instead, we propose to locally approximate the probability of the symbolic constraint under the <strong>pseudolikelihood</strong> distribution&#8212;the product
                of its full conditionals given a sample from the model.
                <!-- -->
                This allows our pseudo-semantic loss function to  enforce the symbolic constraint.
                <!-- -->
                Our method bears relationship to several classical approximation schemes, including hogwild Gibbs sampling, consistent pseudolikelihood learning, and contrastive divergence.
                We test our proposed approach on three distinct settings: Sudoku, shortest-path prediction, and detoxifying 
                large language models. Experiments show that pseudo-semantic loss  greatly improves upon the base model's ability to satisfy the desired logical
                constraint in its output distribution.
              </p>
          </div>
          <p></p>
        </li>


        <li><b><a href="https://web.archive.org/web/20240430001039/https://arxiv.org/abs/2311.13718">A Unified Approach to Count-Based Weakly-Supervised Learning</a></b>
          <p class="content"> Vinay Shukla, Zhe Zeng*, <strong>Kareem Ahmed*</strong>, Guy Van den Broeck </p>
            <!-- <br> -->
            <em>NeurIPS 2023</em>.
          <br>
          [<a href="https://web.archive.org/web/20240430001039/http://web.cs.ucla.edu/~ahmedk/" onclick="$('#count-based_abstract').toggle();return false;">abstract</a>]
          <div id="count-based_abstract" class="abstract" style="display:none;">
              <p>
                We observe that in weakly supervised learning for binary classification, the weak supervision can be seen as a constraint on label counts.
                Inspired by this observation, we aim to develop a unified approach to count-based weakly supervised learning.
                <!-- -->
                From first principles, we derive objectives for three of the most common weakly supervised learning paradigms for binary classification
                and show that these objectives share the same computational building block, which is the probability of an arithmetic constraint defined
                over the count of positive instances, i.e., the summation of binary labels. We further derive an algorithm to compute the count 
                probabilities in an exact and tractable way, which allows our proposed objectives to be integrated into the end-to-end training of neural
                network models. Thorough empirical evaluation is performed over all three weakly supervised learning paradigms and shows that our proposed
                objectives obtained by exact and tractable computations are able to achieve state-of-the-art or highly comparable results on various tasks.
                We also provide some experimental analysis to illustrate that the models learn effectively by our proposed objectives.
              </p>
          </div>
          <p></p>
        </li>

        <li><b><a href="https://web.archive.org/web/20240430001039/https://arxiv.org/abs/2210.01941">SIMPLE: A Gradient Estimator for k-Subset Sampling</a></b>
          <p class="content"> <strong>Kareem Ahmed*</strong>, Zhe Zeng*, Mathias Niepert, Guy Van den Broeck</p>
            <em>ICLR 2023</em>.
            <!-- <strong><span style="color:red">
              Oral Presentation.
            </span></strong> -->
            <br>
            [<a href="https://web.archive.org/web/20240430001039/http://web.cs.ucla.edu/~ahmedk/" onclick="$('#SIMPLE_abstract').toggle();return false;">abstract</a>]
            <div id="SIMPLE_abstract" class="abstract" style="display:none;">
                <p>
                  k-subset sampling is ubiquitous in machine learning, enabling regularization and interpretability through sparsity. The challenge lies
                  in rendering k-subset sampling amenable to end-to-end learning. This has typically involved relaxing the reparameterized samples to allow
                  for backpropagation, with the risk of introducing high bias and high variance. In this work, we fall back to discrete k-subset sampling on
                  the forward pass. This is coupled with using the gradient with respect to the exact marginals, computed efficiently, as a proxy for the true
                  gradient. We show that our gradient estimator, SIMPLE, exhibits lower bias and variance compared to state-of-the-art estimators, including the
                  straight-through Gumbel estimator when k=1. Empirical results show improved performance on learning to explain and sparse linear regression. 
                  We provide an algorithm for computing the exact ELBO for the k-subset distribution, obtaining significantly lower loss compared to SOTA.
                </p>
            </div>
          <p></p>
        </li>

        <li><b><a href="https://web.archive.org/web/20240430001039/https://arxiv.org/abs/2302.14207">Semantic Strengthening of Neuro-Symbolic Learning</a></b>
          <p class="content"> <strong>Kareem Ahmed</strong>, Kai-Wei Chang, Guy Van den Broeck</p>
            <em>AISTATS 2023</em>.
            <!-- <strong><span style="color:red">
              Oral Presentation.
            </span></strong> -->
            <br>
            [<a href="https://web.archive.org/web/20240430001039/http://web.cs.ucla.edu/~ahmedk/" onclick="$('#Semantic-Strengthening_abstract').toggle();return false;">abstract</a>]
            <div id="Semantic-Strengthening_abstract" class="abstract" style="display:none;">
                <p>
                  Numerous neuro-symbolic approaches have recently been proposed typically with the goal of adding symbolic knowledge to the output layer
                  of a neural network. Ideally, such losses maximize the probability that the neural network's predictions satisfy the underlying domain.
                  Unfortunately, this type of probabilistic inference is often computationally infeasible. Neuro-symbolic approaches therefore commonly
                  resort to fuzzy approximations of this probabilistic objective, sacrificing sound probabilistic semantics, or to sampling which is very
                  seldom feasible. We approach the problem by first assuming the constraint decomposes conditioned on the features learned by the network.
                  We iteratively strengthen our approximation, restoring the dependence between the constraints most responsible for degrading the quality
                  of the approximation. This corresponds to computing the mutual information between pairs of constraints conditioned on the network's 
                  learned features, and may be construed as a measure of how well aligned the gradients of two distributions are. We show how to compute 
                  this efficiently for tractable circuits. We test our approach on three tasks: predicting a minimum-cost path in Warcraft, predicting a
                  minimum-cost perfect matching, and solving Sudoku puzzles, observing that it improves upon the baselines while sidestepping intractability.
                </p>
            </div>
          <p></p>
        </li>

        <li><b><a href="https://web.archive.org/web/20240430001039/https://arxiv.org/abs/2206.00426">Semantic Probabilistic Layers for Neuro-Symbolic Learning</a></b>
          <p class="content"> <strong>Kareem Ahmed</strong>, Stefano Teso, Kai-Wei Chang, Guy Van den Broeck, Antonio Vergari</p>
            <em>NeurIPS 2022</em>.
            <!-- <strong><span style="color:red">
              Oral Presentation.
            </span></strong> -->
            <br>
            [<a href="https://web.archive.org/web/20240430001039/http://web.cs.ucla.edu/~ahmedk/" onclick="$('#SPL_abstract').toggle();return false;">abstract</a>]
            <div id="SPL_abstract" class="abstract" style="display:none;">
                <p>
                  We design a predictive layer for structured-output prediction (SOP) that can be plugged into any neural network guaranteeing
                  its predictions are consistent with a set of predefined symbolic constraints. Our Semantic Probabilistic Layer (SPL) can model
                  intricate correlations, and hard constraints, over a structured output space all while being amenable to end-to-end learning 
                  via maximum likelihood. SPLs combine exact probabilistic inference with logical reasoning in a clean and modular way, learning 
                  complex distributions and restricting their support to solutions of the constraint. As such, they can faithfully, and efficiently, 
                  model complex SOP tasks beyond the reach of alternative neuro-symbolic approaches. We empirically demonstrate that SPLs outperform 
                  these competitors in terms of accuracy on challenging SOP tasks including hierarchical multi-label classification, pathfinding and 
                  preference learning, while retaining perfect constraint satisfaction.
                </p>
            </div>
          <p></p>
        </li>

        <li><b><a href="https://web.archive.org/web/20240430001039/https://arxiv.org/abs/2201.11250">Neuro-Symbolic Entropy Regularization</a></b>
          <p class="content"> <strong>Kareem Ahmed</strong>, Eric Wang, Kai-Wei Chang, Guy Van den Broeck</p>
            <em>UAI 2022</em>.
            <strong><span style="color:red">
              Oral Presentation.
            </span></strong>
            <br>
            [<a href="https://web.archive.org/web/20240430001039/http://web.cs.ucla.edu/~ahmedk/" onclick="$('#NeSy-Entropy_abstract').toggle();return false;">abstract</a>]
            <div id="NeSy-Entropy_abstract" class="abstract" style="display:none;">
                <p>
                  In structured prediction, the goal is to jointly predict many output variables that together encode a structured object&#8212;
                  a path in a graph, an entity-relation triple, or an ordering of objects. Such a large output space makes learning hard and requires 
                  vast amounts of labeled data. Different approaches leverage alternate sources of supervision. One approach&#8212;entropy 
                  regularization&#8212;posits that decision boundaries should lie in low-probability regions. It extracts supervision from unlabeled 
                  examples, but remains agnostic to the structure of the output space. Conversely, neuro-symbolic approaches exploit the knowledge that 
                  not every prediction corresponds to a valid structure in the output space. Yet, they does not further restrict the learned output 
                  distribution. This paper introduces a framework that unifies both approaches. We propose a loss, neuro-symbolic entropy regularization, 
                  that encourages the model to confidently predict a valid object. It is obtained by restricting entropy regularization to the distribution 
                  over only valid structures. This loss is efficiently computed when the output constraint is expressed as a tractable logic circuit. Moreover, 
                  it seamlessly integrates with other neuro-symbolic losses that eliminate invalid predictions. We demonstrate the efficacy of our approach on 
                  a series of semi-supervised and fully-supervised structured-prediction experiments, where we find that it leads to models whose predictions 
                  are more accurate and more likely to be valid.
                </p>
            </div>
          <p></p>
        </li>

        <li><b><a href="https://web.archive.org/web/20240430001039/https://proceedings.mlr.press/v176/ahmed22a/ahmed22a.pdf">PYLON: A PyTorch Framework for Learning with Constraints</a></b>
          <p class="content"> <strong>Kareem Ahmed</strong>,  Tao Li, Thy Ton, Quan Guo, Kai-Wei Chang, Parisa Kordjamshidi, Vivek Srikumar, Guy Van den Broeck, Sameer Singh
            <br>
            <em><a href="https://web.archive.org/web/20240430001039/http://starai.cs.ucla.edu/papers/AhmedAAAI22.pdf">AAAI 2021</a>, <a href="https://web.archive.org/web/20240430001039/https://proceedings.mlr.press/v176/ahmed22a/ahmed22a.pdf">NeurIPS 2021</a></em>
          <br>
            [<a href="https://web.archive.org/web/20240430001039/http://web.cs.ucla.edu/~ahmedk/" onclick="$('#Pylon_abstract').toggle();return false;">abstract</a>]
            </p><div id="Pylon_abstract" class="abstract" style="display:none;">
                <p>
                  Deep learning excels at learning low-level task information from large amounts of data, but struggles with learning high-level domain knowledge, 
                  which can often be directly and succinctly expressed. In this work, we introduce Pylon, a neuro-symbolic training framework that builds on PyTorch 
                  to augment procedurally trained neural networks with declaratively specified knowledge. Pylon allows users to programmatically specify constraints 
                  as PyTorch functions, and compiles them into a differentiable loss, thus training predictive models that fit the data whilst satisfying the specified 
                  constraints. Pylon includes both exact as well as approximate compilers to efficiently compute the loss, employing fuzzy logic, sampling methods, 
                  and circuits, ensuring scalability even to complex models and constraints. A guiding principle in designing Pylon has been the ease with which any 
                  existing deep learning codebase can be extended to learn from constraints using only a few lines: a function expressing the constraint and a single 
                  line of code to compile it into a loss. We include case studies from natural language processing, computer vision, logical games, and knowledge graphs, 
                  that can be interactively trained, and highlights Pylon's usage.
                </p>
            </div>
           <p></p>
        </li>

        <li><b><a href="https://web.archive.org/web/20240430001039/https://arxiv.org/abs/2103.11062">Leveraging Unlabeled Data for Entity-Relation Extraction through Probabilistic Constraint Satisfaction</a></b>
          <p class="content"> <strong>Kareem Ahmed</strong>, Eric Wang, Kai-Wei Chang, Guy Van den Broeck</p>
            <em>arxiv pre-print 2021</em>.
            <!-- <strong><span style="color:red">
              Oral Presentation.
            </span></strong> -->
            <br>
            [<a href="https://web.archive.org/web/20240430001039/http://web.cs.ucla.edu/~ahmedk/" onclick="$('#NeSy-Entropy_abstract').toggle();return false;">abstract</a>]
            <div id="NeSy-Entropy_abstract" class="abstract" style="display:none;">
                <p>
                  We study the problem of entity-relation extraction in the presence of symbolic domain knowledge. Such knowledge takes the 
                  form of an ontology defining relations and their permissible arguments. Previous approaches set out to integrate such 
                  knowledge in their learning approaches either through self-training, or through approximations that lose the precise meaning 
                  of the logical expressions. By contrast, our approach employs semantic loss which captures the precise meaning of a logical 
                  sentence through maintaining a probability distribution over all possible states, and guiding the model to solutions which 
                  minimize any constraint violations. With a focus on low-data regimes, we show that semantic loss outperforms the baselines 
                  by a wide margin.
                </p>
            </div>
          <p></p>
        </li>
    </ul></div>
    
    <div class=" my-4 text-left">
      <h2>Book Chapters</h2>
      <ul>
          <li><i>Semantic Loss Functions for Neuro-Symbolic Structured Prediction.</i>
              <p class="content"><b>Kareem Ahmed</b>, Stefano Teso, Paolo Morettin,
              Luca Di Liello, Pierfrancesco Ardino,<br> Jacopo Gobbi, Yitao Liang,
              Eric Wang, Kai-Wei Chang, Andrea Passerini, and Guy Van den Broeck</p>
              <em>In Compendium of Neurosymbolic AI, pages 485–505. IOS Press, 2023.</em>
          <p></p>
        </li>
      </ul>
    </div>

    <div class=" my-4 text-left">
      <h2>Talks and Presentations</h2>
      <ul>
          <li><i>Neuro-Symbolic Artificial Intelligence: A Probabilistic Perspective.</i>
        <p class="content">Invited lecture at Probabilistic Programming and Relational Learning, UCLA Spring 2023.</p>
          <p></p>
        </li>
          <li><i>Neuro-Symbolic Entropy Regularization.</i>
              <p class="content">Uncertainty in Artificial Intelligence, 2022. <b>Oral full presentation.</b></p>
          <p></p>
        </li>
          <li><i>Pylon: A Pytorch Framework for Learning with Constraints.</i>
              <p class="content">Neural Information Processing, 2022. <b>Oral full presentation.</b></p>
          <p></p>
        </li>
      </ul>
    </div>

    <div class=" my-4 text-left">
      <h2>Service</h2>
      <ul style="list-style-type:none;">
          <li><strong>Program Committee</strong></li> 
            <ul>
                <li>AKBC 2021, TPM 2022, AAAI 2023, UAI 2023, NeurIPS 2023, CLEAR 2023, TPM 2023, DAE 2023, KLR 2023, ICLR 2024</li>
            </ul>
            <p></p>
            <li><strong>Community and Diversity:</strong></li> 
              <ul>
                  <li>Co-founded Computer Science PhD @ UCLA Mentorship Program
              </ul>
      </ul>
    </div>

    <div class=" my-4 text-left">
      <h2>Teaching</h2>
      <ul style="list-style-type:none;">
          <li><strong>Teaching Assistant</strong>: 
            <ul>
                <li>Teaching Assistant for Introduction to Computer Science I and II at UCLA, Fall 2018 - Fall 2022</li>
                <li>Junior Teaching Assistant for Introduction to Programming at GUC, Spring 2013</li>
                <li>Junior Teaching Assistant for Media and Networks Lab at GUC, Spring 2014</li>
            </ul>
            <p></p>
          <li><strong>Guest Lecturer</strong>: 
              <ul>
                  <li>CS267A Probabilistic Programming and Relational Learning at UCLA, Spring 2023
              </ul>
      </ul>
    </div>


    <div class=" my-4 text-left">
      <h2>Research Grants</h2>
      <ul style="list-style-type:none;">
          <li>Contributed largely in writing a DARPA grant proposal
              on <em>Assured Neuro Symbolic Learning and Reasoning</em> largely
              based <br> on my research contributions. We were awarded the
              grant, and I am currently co-leading this cross-site project.
      </ul>
    </div>

    <div class=" my-4 text-left">
      <h2>Mentoring</h2>
      <ul>
          <li>Eric Wang (currently at Citadel)
          <li>Vinay Shukla (UCLA undergraduate)
          <li>Andrei Molanche (PhD Student at University of Stuttgart)
          <li>Poorva Garg (PhD Student at UCLA)
          <li>Renato Geh (PhD Student at UCLA)
          <li>Alexander Chen (UCLA undergraduate)
          <li>Xinran Fang (UCLA undergraduate)
          <li>Tang Mohan (UCLA Master’s)
      </ul>
    </div>


    <!-- Content Row -->
    <!-- /.row -->

  </div>
  <!-- /.container -->

  <!-- Bootstrap core JavaScript -->
  <script src="files/jquery.min.js"></script>
  <script src="files/bootstrap.bundle.min.js"></script>




</body></html>
<!--
     FILE ARCHIVED ON 00:10:39 Apr 30, 2024 AND RETRIEVED FROM THE
     INTERNET ARCHIVE ON 01:23:26 Sep 23, 2024.
     JAVASCRIPT APPENDED BY WAYBACK MACHINE, COPYRIGHT INTERNET ARCHIVE.

     ALL OTHER CONTENT MAY ALSO BE PROTECTED BY COPYRIGHT (17 U.S.C.
     SECTION 108(a)(3)).
-->
<!--
playback timings (ms):
  captures_list: 1.175
  exclusion.robots: 0.267
  exclusion.robots.policy: 0.253
  esindex: 0.023
  cdx.remote: 8.493
  LoadShardBlock: 171.717 (3)
  PetaboxLoader3.datanode: 125.472 (5)
  PetaboxLoader3.resolve: 214.513 (3)
  load_resource: 202.373
  loaddict: 67.433
-->